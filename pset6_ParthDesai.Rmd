---
title: "pset6_ParthDesai"
author: "Parth Desai"
date: "2023-04-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1
```{r prelim}
library(stringr)
emails <- read.csv('Emails.csv', stringsAsFactors = FALSE)
```

## Part 1.1
```{r part1.1}
colnames(emails)
```
#### Column 22 has the raw text

## Part 1.2
```{r part1.2}
email_1 <- str_replace_all(emails[1,22], '[^[:alnum:]]+', ' ')
email_1 <- str_replace_all(email_1, '\\s+', ' ')
```

## Part 1.3
```{r part1.3}
email_vector <- strsplit(email_1, ' ')
```

## Part 1.4
```{r part1.4}
length(email_vector[[1]])
```

# Question 2

## Part 2.1
```{r part2.1}
benghazi_mention <- c()
benghzi_count <- str_which(emails[,22], fixed('benghazi', ignore_case = TRUE))
for (i in 1:nrow(emails)) {
  trial <- as.vector(strsplit(emails[benghzi_count[i],22], ' '))
  benghazi_mention[i] <- length(trial[[1]][str_which(trial[[1]], fixed('benghazi', ignore_case = TRUE))])
}

head(benghazi_mention, n = 5)
tail(benghzi_count, n = 5)

```

## Part 2.2
```{r part2.2}
benghazi_cleaned <- c()
benghazi_output <- c()
for (i in 1:length(benghzi_count)) {
  benghazi_cleaned[i] <- str_replace_all(emails[benghzi_count[i], 22], '[^[:alnum:]\\s]+', '')
  benghazi_cleaned[i] <- str_replace_all(benghazi_cleaned[i], '\\s+', ' ')
  benghazi_cleaned[i] <- tolower(benghazi_cleaned[i])
}

benghazi_regex <- "\\b(\\w+\\s+\\w+\\s+)?benghazi(\\s+\\w+\\s+\\w+)?\\b"
benghazi_matches <- regmatches(benghazi_cleaned, gregexpr(benghazi_regex, benghazi_cleaned))
benghazi_output <- lapply(benghazi_matches, function(matches) unlist(matches))


benghazi_output[[2]]
benghazi_output[length(benghzi_count)]
```

## Part 2.3

#### Benghazi is mentioned when discussing a course of action the House of Representatives will take.

# Question 3
```{r part3prelim}
pos_words <- read.delim("positive-words.txt", header = F, stringsAsFactors = F)[,1]
neg_words <- read.delim("negative-words.txt", header = F, stringsAsFactors = F)[,1]
```

## Part 3.1
```{r part3.1}
email_clean <- c()
clean_split <- c()
pos_count <- c()
neg_count <- c()
for (i in 1:nrow(emails)) {
  email_clean[i] <- str_replace_all(emails[i, 22], '[[:punct:]]', ' ')
  email_clean[i] <- str_replace_all(email_clean[i], '\\s+', ' ')
  email_clean[i] <- tolower(email_clean[i])
  clean_split <- strsplit(email_clean[i], ' ')[[1]]
  pos_count[i] <- sum(clean_split %in% pos_words)
  neg_count[i] <- sum(clean_split %in% neg_words)
}

head(pos_count, n = 5)
tail(neg_count, n = 5)

```
## Part 3.2
```{r part3.2}
sent_frame <- data.frame('Benghazi' = benghazi_mention, 'Positive' = pos_count, 'Negative' = neg_count)
ratio <- c()

for (i in 1:nrow(sent_frame)) {
  if((pos_count[i] == 0) && (neg_count[i] == 0)){
    ratio[i] <- 0.5
  }
  else{
    ratio[i] <- ((pos_count[i])/(pos_count[i] + neg_count[i]))
  }
}

regress <- lm(ratio ~ Benghazi, data = sent_frame)
summary(regress)
```

# Question 4

## Part 4.1
```{r part4.1}
load('DTM.RData')
row_lengths = apply(dtm_use, 1, function(z) sqrt(sum(z^2)))
dtm_norm = dtm_use/row_lengths
```

## Part 4.2
```{r part4.2}
K <- 3
set.seed(12345)
K3_norm <- kmeans(dtm_norm, centers = K)
prop.table(table(K3_norm$cluster))
```

## Part 4.3
```{r part4.3}
K2 <- 6
set.seed(12345)
K6_og <- kmeans(dtm_use, centers = K2)
set.seed(12345)
K6_og_norm <- kmeans(dtm_norm, centers = K2, nstart = 3)
prop.table(table(K6_og$cluster))
prop.table(table(K6_og_norm$cluster))

top_words_unnorm = lapply(1:6, function(i) {
  cluster <- K6_og$cluster == i
  words <- colnames(dtm_use)[cluster]
  freq <- rowSums(dtm_use[,cluster])
  mean_freq <- mean(freq)
  top_freq_words <- head(sort(freq, decreasing = TRUE), 10)
  top_diff_words <- head(sort(freq - mean_freq, decreasing = TRUE), 10)
  list(unorm_top_freq_words = top_freq_words, unorm_top_diff_words = top_diff_words)
})

top_words_norm = lapply(1:6, function(i) {
  cluster <- K6_og_norm$cluster == i
  words <- colnames(dtm_use)[cluster]
  freq <- rowSums(dtm_use[,cluster])
  mean_freq <- mean(freq)
  top_freq_words <- head(sort(freq, decreasing = TRUE), 10)
  top_diff_words <- head(sort(freq - mean_freq, decreasing = TRUE), 10)
  list(norm_top_freq_words = top_freq_words, norm_top_diff_words = top_diff_words)
})

top_words <- data.frame(top_words_unnorm, top_words_norm)
top_words
```

## Part 4.4

#### Each cluster captures the most frequent and unique word choice of a given email compared to its 3 or 6 closest neighbors. I think the normalized document term matrix is more meaningful as the document length is held more constantly. This gives an equal length which to compare all documents that is not provided by the original document term matrix.

